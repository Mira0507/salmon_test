---
output: html_document
---

```{r global_options, include=FALSE}

knitr::opts_chunk$set(
    warning=FALSE,
    message=FALSE
    )

```

## Loading packages

```{r loading_packages}
library(data.table)
library(rmarkdown)
library(AnnotationHub)
library(tidyverse)
library(tximport)
library(ggplot2)
library(DESeq2)
library(pheatmap)
```

## Setting AnnotationHub

Assign your species of interest

```{r annotationhub_setup}

AnnotationSpecies <- "Homo sapiens"  # Assign your species 
ah <- AnnotationHub(hub=getAnnotationHubOption("URL"))   # Bring annotation DB

```

## Running AnnotationHub 

```{r run_annotationhub}

ahQuery <- query(ah, c("OrgDb", AnnotationSpecies))      # Filter annotation of interest

if (length(ahQuery) == 1) {
    DBName <- names(ahQuery)
} else if (length(ahQuery) > 1) {
               DBName <- names(ahQuery)[1]
} else {
    print("You don't have a valid DB")
    rmarkdown::render() 
} 

AnnoDb <- ah[[DBName]] # Store into an OrgDb object  


# Explore your OrgDb object with following accessors:
# columns(AnnpDb)
# keytypes(AnnoDb)
# keys(AnnoDb, keytype=..)
# select(AnnoDb, keys=.., columns=.., keytype=...)
AnnoKey <- keys(AnnoDb, keytype="ENSEMBLTRANS")

# Note: Annotation has to be done with not genome but transcripts 
AnnoDb <- select(AnnoDb, 
                 AnnoKey,
                 keytype="ENSEMBLTRANS",
                 columns="SYMBOL")

```


## Checking out the AnnotationHub output 

```{r checking_annotationhub_output}

# Check if your AnnoDb has been extracted and saved correctely
class(AnnoDb)
head(AnnoDb)

```

## Defining file name and path for .sf files

.sf files have been created from fastq data by salmon


```{r preparing_importing.sf}

# This code chunk needs to be written by yourself 

# Define sample names 
SampleNames <-  c("Mock_72hpi_S1",
                 "Mock_72hpi_S2",
                 "Mock_72hpi_S3",
                 "SARS-CoV-2_72hpi_S7",
                 "SARS-CoV-2_72hpi_S8",
                 "SARS-CoV-2_72hpi_S9") 

# Define group level
GroupLevel <- c("Mock", "COVID")

# Define contrast for DE analysis
Contrast <- c("Group", "COVID", "Mock")


# Set a directory to save csv files
dir.create("./csv")


# Define .sf file path
sf <- c(paste0(SampleNames,
               ".fastq.gz.salmon_quant/quant.sf"))

# Define sample groups
group <- c(rep("Mock", 3), rep("COVID", 3))

# Create metadata
metadata <- data.frame(Sample=factor(SampleNames, levels=SampleNames),
                       Group=factor(group, levels=GroupLevel),
                       Path=sf)

rownames(metadata) <- SampleNames


# Explore the metadata
print(metadata)
```

## Converting .sf files to txi list 

- txi_tpm: stores **TPM** with the argument **"countsFromAbundance="lengthScaledTPM"**
- txi_counts: stores **original counts** 
- We could alternatively generate counts from abundances, using the argument **countsFromAbundance**, scaled to library size, **"scaledTPM"**, or additionally scaled using the average transcript length, averaged over samples and to library size, **"lengthScaledTPM"**. Using either of these approaches, the counts are not correlated with length, and so the length matrix should not be provided as an offset for downstream analysis packages. For more details on these approaches, see the article listed under citation("tximport").
- If you don't want gene-level summarization, set below argument:
  **txOut=TRUE**. 
- Visit tximport doc: https://bioconductor.riken.jp/packages/3.4/bioc/vignettes/tximport/inst/doc/tximport.html


### Discussion about TPM as an input of DESeq2    
#### from [Bioinformatics Forum](https://www.biostars.org/p/411259/))    
None of it is ideal. You should start from raw counts if possible. If not and TPM is the only thing you have then what does it matter, results will anyway be unreliable, so interpret them with care and try to validate the important genes with either experiments or published RNA-seq data in a similar setup. Gene length correction down-penalizes counts of long genes and by this reduces its power, this is (from what I understand) why it is not by default done in tools like edgeR or DESeq2. But as said, if TPM is what you have with no alternative, go and try the pipeline suggested at BioC and then see if you get anything out of it. Maybe you already have genes from which you know that they must / should change in your setup, and some that should not, so you can get an idea how robust the results are. You should also think about how drastic you expect changes to be. TPM cannot correct for library composition (see e.g. on Youtube the StatQuest videos on DESeq2 and edgeR library normalization to learn what this is and why it is important to correct for it), so if you expect major changes in the transcriptional landscape like mostly genes being upregulated or vice verse, your TPM counts might be skewed. Again, be careful with the results and try to confirm important genes that you want to build on.

```{r saving_reads_to_dataframe}

# Assign sample names to the input (.sf) file path
names(sf) <- SampleNames

# Run tximport
# tpm vs original counts
# input sf: a factor of all .sf files' path
txi_tpm <- tximport(sf, 
                    type="salmon",
                    tx2gene=AnnoDb,
                    countsFromAbundance="lengthScaledTPM", # Extracts TPM 
                    ignoreTxVersion=T) 

txi_counts <- tximport(sf, 
                    type="salmon",
                    tx2gene=AnnoDb,
                    ignoreTxVersion=T) 

```

## Exploring the txi outputs 

```{r txi_outputs}

# tpm 
head(txi_tpm$counts)
dim(txi_tpm$counts)

# counts
head(txi_counts$counts)
dim(txi_counts$counts)

```


## Creating an DESeq object from txi and VST

- Note: The tximport-to-DESeq2 approach uses estimated gene counts from the transcript abundance quantifiers, but not normalized counts.
- The **txi input** was used to create a DESeq object (aka **dds**) with **DESeqDataSetFromTximport()** function 
- **vst()** was run for variance stabilizing transformation instead of rlog which takes longer time with similar characteristics. 
- The **vsd** object created by vst() is used for not DE analysis but QC. 
- Visit DESeq2 doc: http://bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html


```{r creating_dds_vsd}

# Set a function creating dds and vsd
dds_vsd_fn <- function(txi) { 

    # Create a DESeq object (so-calledd dds) 
    des <- DESeqDataSetFromTximport(txi, 
                                    colData=metadata,
                                    design=~Group)

    # Create a vsd object (so-called vsd) 
    ves <- vst(des, blind=T)

    # Output them as a list 
    return(list(dds=des, vsd=ves))
}

TPM <- dds_vsd_fn(txi_tpm)
Counts <- dds_vsd_fn(txi_counts)

# Outputs
# dds: TPM/Counts[[1]] or TPM/Counts[['dds']] 
# vsd: TPM/Counts[[2]] or TPM/Counts[['vsd']]

```


## Exploring created dds 

```{r exploring_dds}

# TPM 
TPM[[1]]
head(counts(TPM[[1]]))


# Counts
Counts[[1]]
head(counts(Counts[[1]]))
```

## Exploring created vsd

```{r exploring_vsd}

# TPM
TPM[[2]]


# Counts
Counts[[2]]
```


## Estimating size factors, dispersions, and conducting the Wald Test

- **Dispersion** is calculated as a **measure of variation** instead of variance as variance gets larger when gene expression gets higher. 
- **Wald test** is the default setting of DESeq2 testing null hypothesis when you compare **two groups**. Use **Likelihood ratio test (LRT)** when you have compare **more than two groups**. 

```{r DESeq_prep}


# Set a function estimating size factors, dispersions, and perform wald test
DESeqPrep_fn <- function(List) {
    
    List[[1]] <- estimateSizeFactors(List[[1]])
    List[[1]] <- estimateDispersions(List[[1]])
    List[[1]] <- nbinomWaldTest(List[[1]])
   
    return(List)
}

# Update dds with the function
Counts <- DESeqPrep_fn(Counts) 
TPM <- DESeqPrep_fn(TPM)


```

## Exploring size factors

```{r exploring_sizefactors}


sizeFactors(Counts[[1]])
sizeFactors(TPM[[1]])

# Size factors don't exist in the Counts dds!
# Normalization factors are calculated in the Counts dds instead! 
assays(Counts[[1]])
assays(TPM[[1]])

colData(Counts[[1]])
colData(TPM[[1]])

```

## Plotting the size factors in TPM

- The size factors are only available from TPM dds 


```{r plotting_sizefactors}


# Extract and save the size factors in a data frame
sizeFactor <- as.data.frame(round(sizeFactors(TPM[[1]]), 3))

colnames(sizeFactor) <- 'Size_Factor'
sizeFactor <- sizeFactor %>%
    rownames_to_column(var="Sample") %>%
    inner_join(metadata[, 1:ncol(metadata)-1], by="Sample") 

# Create a plot comparing the size factors by sample
ggplot(sizeFactor, aes(x=Sample, 
                       y=Size_Factor, 
                       fill=Group,
                       label=Size_Factor)) +
    geom_bar(stat="identity", width=0.8) +
    theme_bw() + 
    ggtitle("Size Factors in TPM-DESeq") +
    geom_text(vjust=1.5) +
    theme(axis.text.x=element_text(angle=45, 
                                   vjust=0.5)) + ylab("Size Factor")
    



```


## Plotting nornalization factors in the Counts

- DESeq2 performs an internal normalization where geometric mean is calculated for each gene across all samples. The counts for a gene in each sample is then divided by this mean. The median of these ratios in a sample is the size factor for that sample.


```{r plotting_normalizationfactors}

# Extract and save normalization factors in a data frame
normf <- as.data.frame(normalizationFactors(Counts[[1]])) %>%
    gather(Sample, Normalization_Factor) %>%
    inner_join(metadata[, 1:2], by="Sample") 

normf$Sample <- as.factor(normf$Sample, levels=SampleNames)
normf$Group <- as.factor(normf$Group, levels=GroupLevel)


ggplot(normf, 
       aes(x=Sample, y=Normalization_Factor)) + 
geom_jitter(alpha=0.5, aes(color=Group)) + 

# Add a boxplot to provide statistics in each sample
geom_boxplot(aes(x=Sample, y=Normalization_Factor), 
             outlier.shape=NA) + 
theme_bw() +
ggtitle("Normalization Factors") +
theme(axis.text.x=element_text(angle=45, 
                               vjust=0.5)) + 
ylab("Normalization Factor / Gene") +

# Add a dashed horizontal line to indicate where normalization factor=1
geom_hline(yintercept=1, 
           color="blue", 
           linetype="dashed")





```


## Setting functions for GC plots

```{r QCplot_functions}

# Assigne what to compare
GroupOfInterest <- Contrast[1] 

# Set a function for a PCA plot
QCPCA_fn <- function(inputList, Title) {

    plotPCA(inputList[[2]],
            intgroup=GroupOfInterest) + theme_bw() + ggtitle(Title)

}


# Set a function for a correlation heatmap 
GCcorrHeatmap_fn <- function(inputList, Title) {

    # Extract transformed count matrix
    mtx <- assay(inputList[[2]])

    # Calculate correlation and store in the matrix
    mtx <- cor(mtx)

    # Set heatmap annotation 
    ColOfInterest <- !colnames(metadata) %in% c("Sample", "Path")
    HeatmapAnno <- as.data.frame(metadata[, ColOfInterest])
    rownames(HeatmapAnno) <- SampleNames

    # Create a correlation heatmap
    return(pheatmap(mtx, 
             annotation=HeatmapAnno,
             main=paste("Sample Correlation Heatmap:",
                        Title)))

}

```


## Sample QC: Principal Component Analysis 




```{r QC_PCA}


# TPM
QCPCA_fn(TPM, "QC PCA: TPM")

# Counts
QCPCA_fn(Counts, "QC PCA: Counts")


```




## Sample QC: Sample Correlation Heatmap


```{r QC_correlation_heatmap}
mtx <- assay(Counts[[2]])

    # Calculate correlation and store in the matrix
    mtx <- cor(mtx)

    # Set heatmap annotation 
    ColOfInterest <- !colnames(metadata) %in% c("Sample", "Path")
    HeatmapAnno <- as.data.frame(metadata[, ColOfInterest])
    rownames(HeatmapAnno) <- SampleNames

    # Create a correlation heatmap
    pheatmap(mtx, 
             annotation=HeatmapAnno,
             main=paste("Sample Correlation Heatmap"))



GCcorrHeatmap_fn(Counts, "TPM") 
```

## Running DE analysis


```{r DE_analysis}

# Run DESeq 
dds <- DESeq(dds)


# Check result names 
ResNames <- resultsNames(dds)
print(ResNames)

```

## Creating a dispersion plot

```{r dispersion_plot}

plotDispEsts(dds)


```

## Setting how to extract fold-change results
### Change variables below

```{r setting_resultcondition}


# Set the threshold of FDR as a variable "alpha" 
alpha=0.1

# Set the coefficients to compare 
Coef <- ResNames[-1]
print(Coef) 


# Set a function to clean result table 
LFCTable_fn <- function(df) {

    df <- df %>% 
        rownames_to_column(var="Annotation") %>%
        separate("Annotation", c("Transcript", "Gene")) %>%
        mutate(FDR=ifelse(padj < 0.1 & !is.na(padj), 
                                   "< 0.1", 
                                   "> 0.1")) 

    return(df)
}
```

## Extracting log2FoldChanges
### You can change alpha depending on your interest of FDR level



```{r DEresult_extraction}

# Extract DE results
# The Contrast variable was defined in the preparing_importing.sf chunk



# Extraction with no shrinkage
# alpha: FDR threshold
Res <- results(dds, contrast=Contrast, alpha=alpha)

# Convert the LFC data to a data frame
ResDF <- LFCTable_fn(as.data.frame(Res))


# Save the LFC data 
write.csv(ResDF, "./csv/Read_LFC_noshrinkage.csv")

# Extracttion without shrinkage in a list 
# name has to be determined in the previous chunk
resList <- list()

for (i in 1:length(Coef)) {

    myresult <- lfcShrink(dds,
                          coef=Coef[i],
                          type="apeglm")
    resList[i] <- myresult
}



```

## Determining what comparison to explore 
### Checkout resList in the previous chunk and save it to a data frame 


```{r LFC_to_dataframe}

# Save data of interest as a data frame for further analysis 
shRes <- as.data.frame(resList[[1]])

# Clean the LFC table 
shRes <- LFCTable_fn(shRes)

# Save the LFC table
write.csv(shRes, "./csv/Read_LFC_shrinkage.csv")

head(shRes)
```

## Exploring distribution of false discovery rate (FDR)

```{r FDR_distribution}

# Create a plot presenting distribution of FDR
FDR_distPlot <- 
    ggplot(shRes,
           aes(x=padj)) + 
geom_density() + 
theme_bw() +
ggtitle("Distribution of False Discovery Rate (FDR)") + 
xlab("Adjusted P-Value") + 
ylab("Density") + 
geom_vline(xintercept=alpha, color="red")

# Print the plot
print(FDR_distPlot)


```

## Exploring FDR statistics

### - NumOfTx: total number of transcripts
### - BelowAlpha: number of transcripts whose FDR is below 0.1
### - NumofNAInFDR: number of transcripts whose FDR is NA
### - PropOfAboveAlpha: BelowAlpha/NumOfTx 


```{r FDR_statistics}

FDR_stat <- data.frame(NumOfTx=nrow(shRes),
                       BelowAlpha=sum(shRes$FDR == "< 0.1"),
                       NumOfNAInFDR=sum(is.na(shRes$padj)), 
                       PropOfAboveAlpha=sum(shRes$FDR == "< 0.1") / nrow(shRes))

head(FDR_stat)
```

## Exploring distribution of log2FoldChange

### Black: total transcripts (padj =/= NA)
### Colored: transcripts above or below FDR=0.1


```{r L2FC_distribution}

# Subset transcripts whose padj are not NA
shRes_nonNa <- subset(shRes, !is.na(padj))


L2FC_dist <- 
    ggplot(shRes_nonNa,
           aes(x=log2FoldChange)) + 
geom_density(color="black") + 
geom_density(data=shRes_nonNa,
             aes(x=log2FoldChange,
                 color=FDR)) +
theme_bw() +
ggtitle("Distribution of Fold Change Values") + 
ylab("Density")

print(L2FC_dist)

```



## Exploring mean-difference with an MA plot


```{r MAplot}


# Define a function creating an MA plot
MA_fn <- function(df, tit) {

    ggplot(df, 
           aes(x=baseMean,
               y=log2FoldChange,
               color=FDR)) +
geom_point()+ 
scale_x_log10() + 
theme_bw() + 
scale_color_manual(values=c("blue", "grey")) + 
ggtitle(tit)
}


# Create MA plots with or without shrinkage
MAplot_noshr <- MA_fn(ResDF, "Mean-Differene without shrinkage")
MAplot_shr <- MA_fn(shRes, "Mean-Difference with shrinkage")

# Print the plots
print(MAplot_noshr)
print(MAplot_shr)

```

## Volcano plot

```{r volcano_plot}


ggplot(shRes, 
       aes(x=log2FoldChange,
           y= -log10(padj),
           color=FDR)) + 
geom_point() +
theme_bw() +
scale_color_manual(values=c("blue", "grey")) + 
ggtitle("Volcano Plot") + 
ylab("-log10(FDR)")

```

## Exploring transcription profiling (FDR < 0.1)


```{r transcription_profiling_FDR}

# Determine row numbers whose FDR is below alpha 
RowBelowAlpha <- which(shRes$FDR == "< 0.1")

# Subset normalized transcript counts with FDR below alpha 
TranscriptsBelowAlpha <- assay(vsd)[RowBelowAlpha,]

# Create a heatmap from TranscriptsBelowAlpha 
pheatmap(TranscriptsBelowAlpha,
         annotation=HeatmapAnno,
         main="Transcription Profiles (FDR < 0.1)")

```

## Exploring transcription profiling 
## (FDR < 0.1 & log2FoldChange >= 1)


```{r transciption_profiling_FDRandFold}

# Set minimum log2FoldChange of your interest: MinL2F
MinL2F <- 1

# Determine row numbers whose log2FoldChange >= MinL2F
AboveMinL2F <- which(shRes$FDR == "< 0.1" &
                      shRes$log2FoldChange >= MinL2F)


# Subset normalized transcript counts with log2FoldChange above MinL2F 
TranscriptsAboveMinL2F <- assay(vsd)[AboveMinL2F,]


pheatmap(TranscriptsAboveMinL2F,
         annotation=HeatmapAnno,
         main="Transcription Profiles (FDR < 0.1 and log2FoldChange >= 1)",fontsize_row=5)

length(AboveMinL2F)

```



## Session Info 

```{r sessionInfo}
sessionInfo()
```
