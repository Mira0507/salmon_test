---
title: Comparison of TPM and original counts in DE analysis with the DESeq2 package
output: html_document
---

```{r global_options, include=FALSE}

knitr::opts_chunk$set(
    warning=FALSE,
    message=FALSE
    )

```

## Loading packages

```{r loading_packages}
library(data.table)
library(rmarkdown)
library(AnnotationHub)
library(tidyverse)
library(tximport)
library(ggplot2)
library(DESeq2)
library(pheatmap)
library(gridExtra)
library(ggplotify)
library(ggrepel)
```

## Setting AnnotationHub

Assign your species of interest

```{r annotationhub_setup}

AnnotationSpecies <- "Homo sapiens"  # Assign your species 
ah <- AnnotationHub(hub=getAnnotationHubOption("URL"))   # Bring annotation DB

```

## Running AnnotationHub 

```{r run_annotationhub}

ahQuery <- query(ah, c("OrgDb", AnnotationSpecies))      # Filter annotation of interest

if (length(ahQuery) == 1) {
    DBName <- names(ahQuery)
} else if (length(ahQuery) > 1) {
               DBName <- names(ahQuery)[1]
} else {
    print("You don't have a valid DB")
    rmarkdown::render() 
} 

AnnoDb <- ah[[DBName]] # Store into an OrgDb object  


# Explore your OrgDb object with following accessors:
# columns(AnnpDb)
# keytypes(AnnoDb)
# keys(AnnoDb, keytype=..)
# select(AnnoDb, keys=.., columns=.., keytype=...)
AnnoKey <- keys(AnnoDb, keytype="ENSEMBLTRANS")

# Note: Annotation has to be done with not genome but transcripts 
AnnoDb <- select(AnnoDb, 
                 AnnoKey,
                 keytype="ENSEMBLTRANS",
                 columns="SYMBOL")

```


## Checking out the AnnotationHub output 

```{r checking_annotationhub_output}

# Check if your AnnoDb has been extracted and saved correctely
class(AnnoDb)
head(AnnoDb)

```

## Defining file name and path for .sf files

.sf files have been created from fastq data by salmon


```{r preparing_importing.sf}

# This code chunk needs to be written by yourself 

# Define sample names 
SampleNames <-  c("Mock_72hpi_S1",
                 "Mock_72hpi_S2",
                 "Mock_72hpi_S3",
                 "SARS-CoV-2_72hpi_S7",
                 "SARS-CoV-2_72hpi_S8",
                 "SARS-CoV-2_72hpi_S9") 

# Define group level
GroupLevel <- c("Mock", "COVID")

# Define contrast for DE analysis
Contrast <- c("Group", "COVID", "Mock")

# Define a vector for comparing TPM vs Counts effect 
TvC <- c("TPM", "Counts")


# Set a directory to save csv files
dir.create("./csv")


# Define .sf file path
sf <- c(paste0(SampleNames,
               ".fastq.gz.salmon_quant/quant.sf"))

# Define sample groups
group <- c(rep("Mock", 3), rep("COVID", 3))

# Create metadata
metadata <- data.frame(Sample=factor(SampleNames, levels=SampleNames),
                       Group=factor(group, levels=GroupLevel),
                       Path=sf)

rownames(metadata) <- SampleNames


# Explore the metadata
print(metadata)
```

## Converting .sf files to txi list 

- txi_tpm: stores **TPM** with the argument **"countsFromAbundance="lengthScaledTPM"**
- txi_counts: stores **original counts** 
- We could alternatively generate counts from abundances, using the argument **countsFromAbundance**, scaled to library size, **"scaledTPM"**, or additionally scaled using the average transcript length, averaged over samples and to library size, **"lengthScaledTPM"**. Using either of these approaches, the counts are not correlated with length, and so the length matrix should not be provided as an offset for downstream analysis packages. For more details on these approaches, see the article listed under citation("tximport").
- If you don't want gene-level summarization, set below argument:
  **txOut=TRUE**. 
- Visit tximport doc: https://bioconductor.riken.jp/packages/3.4/bioc/vignettes/tximport/inst/doc/tximport.html


### Discussion about TPM as an input of DESeq2    
#### from [Bioinformatics Forum](https://www.biostars.org/p/411259/))    
None of it is ideal. You should start from raw counts if possible. If not and TPM is the only thing you have then what does it matter, results will anyway be unreliable, so interpret them with care and try to validate the important genes with either experiments or published RNA-seq data in a similar setup. Gene length correction down-penalizes counts of long genes and by this reduces its power, this is (from what I understand) why it is not by default done in tools like edgeR or DESeq2. But as said, if TPM is what you have with no alternative, go and try the pipeline suggested at BioC and then see if you get anything out of it. Maybe you already have genes from which you know that they must / should change in your setup, and some that should not, so you can get an idea how robust the results are. You should also think about how drastic you expect changes to be. TPM cannot correct for library composition (see e.g. on Youtube the StatQuest videos on DESeq2 and edgeR library normalization to learn what this is and why it is important to correct for it), so if you expect major changes in the transcriptional landscape like mostly genes being upregulated or vice verse, your TPM counts might be skewed. Again, be careful with the results and try to confirm important genes that you want to build on.

```{r saving_reads_to_dataframe}

# Assign sample names to the input (.sf) file path
names(sf) <- SampleNames

# Run tximport
# tpm vs original counts
# input sf: a factor of all .sf files' path
txi_tpm <- tximport(sf, 
                    type="salmon",
                    tx2gene=AnnoDb,
                    countsFromAbundance="lengthScaledTPM", # Extracts TPM 
                    ignoreTxVersion=T) 

txi_counts <- tximport(sf, 
                    type="salmon",
                    tx2gene=AnnoDb,
                    ignoreTxVersion=T) 

```

## Exploring the txi outputs 

```{r txi_outputs}

# tpm 
head(txi_tpm$counts)
dim(txi_tpm$counts)

# counts
head(txi_counts$counts)
dim(txi_counts$counts)

```


## Creating an DESeq object from txi and VST

- Note: The tximport-to-DESeq2 approach uses estimated gene counts from the transcript abundance quantifiers, but not normalized counts.
- The **txi input** was used to create a DESeq object (aka **dds**) with **DESeqDataSetFromTximport()** function 
- **vst()** was run for variance stabilizing transformation instead of rlog which takes longer time with similar characteristics. 
- The **vsd** object created by vst() is used for not DE analysis but QC. 
- Visit DESeq2 doc: http://bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html


```{r creating_dds_vsd}

# Set a function creating dds and vsd
dds_vsd_fn <- function(txi) { 

    # Create a DESeq object (so-calledd dds) 
    des <- DESeqDataSetFromTximport(txi, 
                                    colData=metadata,
                                    design=~Group)

    # Create a vsd object (so-called vsd) 
    ves <- vst(des, blind=T)

    # Output them as a list 
    return(list(dds=des, vsd=ves))
}

TPM <- dds_vsd_fn(txi_tpm)
Counts <- dds_vsd_fn(txi_counts)

# Outputs
# dds: TPM/Counts[[1]] or TPM/Counts[['dds']] 
# vsd: TPM/Counts[[2]] or TPM/Counts[['vsd']]

```


## Exploring created dds 

```{r exploring_dds}

# TPM 
TPM[[1]]
head(counts(TPM[[1]]))


# Counts
Counts[[1]]
head(counts(Counts[[1]]))
```

## Exploring created vsd

```{r exploring_vsd}

# TPM
TPM[[2]]


# Counts
Counts[[2]]
```


## Estimating size factors, dispersions, and conducting the Wald Test

- **Dispersion** is calculated as a **measure of variation** instead of variance as variance gets larger when gene expression gets higher. 
- **Wald test** is the default setting of DESeq2 testing null hypothesis when you compare **two groups**. Use **Likelihood ratio test (LRT)** when you have compare **more than two groups**.     
- Message when "Counts <- DESeqPrep_fn(Counts)" was run:       
using **'avgTxLength'** from assays(dds), correcting for library size
gene-wise dispersion estimates
mean-dispersion relationship
final dispersion estimates

```{r DESeq_prep}


# Set a function estimating size factors, dispersions, and perform wald test
DESeqPrep_fn <- function(List) {
    
    List[[1]] <- estimateSizeFactors(List[[1]])
    List[[1]] <- estimateDispersions(List[[1]])
    List[[1]] <- nbinomWaldTest(List[[1]])
   
    return(List)
}

# Update dds with the function
Counts <- DESeqPrep_fn(Counts) 
TPM <- DESeqPrep_fn(TPM)


```

## Exploring size factors

```{r exploring_sizefactors}


sizeFactors(Counts[[1]])
sizeFactors(TPM[[1]])

# Size factors don't exist in the Counts dds!
# Normalization factors are calculated in the Counts dds instead! 
assays(Counts[[1]])
assays(TPM[[1]])

colData(Counts[[1]])
colData(TPM[[1]])

```

## Plotting the size factors in TPM

- The size factors are only available from TPM dds 


```{r plotting_sizefactors}


# Extract and save the size factors in a data frame
sizeFactor <- as.data.frame(round(sizeFactors(TPM[[1]]), 3))

colnames(sizeFactor) <- 'Size_Factor'
sizeFactor <- sizeFactor %>%
    rownames_to_column(var="Sample") %>%
    inner_join(metadata[, 1:ncol(metadata)-1], by="Sample") 

# Create a plot comparing the size factors by sample
ggplot(sizeFactor, aes(x=Sample, 
                       y=Size_Factor, 
                       fill=Group,
                       label=Size_Factor)) +
    geom_bar(stat="identity", width=0.8) +
    theme_bw() + 
    ggtitle("Size Factors in TPM-DESeq") +
    geom_text(vjust=1.5) +
    theme(axis.text.x=element_text(angle=45, 
                                   vjust=0.5)) + ylab("Size Factor")
    



```


## Plotting nornalization factors in the Counts

- DESeq2 performs an internal normalization where geometric mean is calculated for each gene across all samples. The counts for a gene in each sample is then divided by this mean. The median of these ratios in a sample is the size factor for that sample.

- Blue dashed line: normalization factor = 1


```{r plotting_normalizationfactors}

# Extract and save normalization factors in a data frame
normf <- as.data.frame(normalizationFactors(Counts[[1]])) %>%
    gather(Sample, Normalization_Factor) %>%
    inner_join(metadata[, 1:2], by="Sample") 

normf$Sample <- factor(normf$Sample, levels=SampleNames)
normf$Group <- factor(normf$Group, levels=GroupLevel)

# Create a scatter plot showing distribution of normalization factors 
ggplot(normf, 
       aes(x=Sample, y=Normalization_Factor)) + 
geom_jitter(alpha=0.5, aes(color=Group)) + 

# Add a boxplot to provide statistics in each sample
geom_boxplot(aes(x=Sample, y=Normalization_Factor), 
             outlier.shape=NA) + 
theme_bw() +
ggtitle("Normalization Factors") +
theme(axis.text.x=element_text(angle=45, 
                               vjust=0.5)) + 
ylab("Normalization Factor / Gene") +

# Add a dashed horizontal line to indicate where normalization factor=1
geom_hline(yintercept=1, 
           color="blue", 
           linetype="dashed")





```


## Setting functions for GC plots

```{r QCplot_functions}

# Assigne what to compare
GroupOfInterest <- Contrast[1] 

# Set a function for a PCA plot
QCPCA_fn <- function(inputList, Title) {

    plotPCA(inputList[[2]],    # takes vsd
            intgroup=GroupOfInterest) + theme_bw() + ggtitle(Title)

}

# Set heatmap annotation 
ColOfInterest <- !colnames(metadata) %in% c("Sample", "Path")
HeatmapAnno <- as.data.frame(metadata[, ColOfInterest])
rownames(HeatmapAnno) <- SampleNames
colnames(HeatmapAnno) <- colnames(metadata)[ColOfInterest]

# Set a function for a correlation heatmap 
QCcorrHeatmap_fn <- function(inputList, Title) {

    # Extract transformed count matrix
    mtx <- assay(inputList[[2]])      # takes vsd

    # Calculate correlation and store in the matrix
    mtx <- cor(mtx)

    
    
    # Create a correlation heatmap
    return(pheatmap(mtx, 
             annotation=HeatmapAnno,
             main=paste("Sample Correlation Heatmap:",
                        Title)))

}

```


## Sample QC: Principal Component Analysis 

- Checkpoints in PCA: source of variation, sample outlier



```{r QC_PCA}


grid.arrange(QCPCA_fn(TPM, "QC PCA: TPM"), 
             QCPCA_fn(Counts, "QC PCA: Counts"), 
             nrow=2)
```




## Sample QC: Sample Correlation Heatmap

- Checkpoint of correlation heatmap: distance between samples, correlation in a group

```{r QC_correlation_heatmap}

# TPM
QCcorrHeatmap_fn(TPM, "TPM") 
# Counts
QCcorrHeatmap_fn(Counts, "Counts") 



```

## Running DE analysis


```{r DE_analysis}

# Create a list for TPM and Counts dds 
ddsList <- list(TPM=TPM[[1]], Counts=Counts[[1]]) 

for (x in TvC) {

    # Run DESeq() 
    ddsList[[x]] <- DESeq(ddsList[[x]])
    print(resultsNames(ddsList[[x]]))
}



```

## Creating a dispersion plot

- Dispersion is important since estimation by DESeq2 algorithm is based on the assumption that genes with similar expression levels have similar dispersion. If an RNA-seq dataset doesn't satisfy this assumption, use other DE algorithm than DESeq2. 


```{r dispersion_plot}


# Plot dispersion  

for (x in TvC) {

    plotDispEsts(ddsList[[x]], 
                 ylab="Dispersion", 
                 xlab="Mean of Normalized Counts", 
                 main=paste("Dispersion of", x, "Input"))
}

```


## Performing shrinkage 

- Shrinkage is beneficial to reduce false positives 
- Magnitude of shrinkage is affected by dispersion and sample size
- When the **type** is set to **"apeglm"**, the **coef** argument is used instead of constrast. In this dataset, you can set **coef=Coef** where **Coef <- resultsNames(ddsList[[1]])**. 
- When the **type** is set to **"normal"**, the **contrast** is set as shown below. 


```{r shrinkage}

# Create an empty list for shrunken dds
shr_ddsList <- list(TPM=c(), Counts=c()) 

for (x in TvC) {

    # shrink
    shr_ddsList[[x]] <- lfcShrink(ddsList[[x]], 
                                  contrast=Contrast, # contrast  
                                  type="normal")     # is paired with "normal" type
}


```


# Extracting log2FoldChange and p-values with or without shrinkage

- The **alpha** denotes threshold of **false discovery rate (FDR)** assigned by users. 
- In this analysis, alpha is set to 0.1 


```{r extracting_log2FoldChanges}

# Set FDR threshold 
alpha=0.1 

# Set a function cleaning table
Sig_fn <- function(df, Input) {

    df <- df %>% 
        rownames_to_column(var="Gene") %>%
        mutate(FDR=ifelse(padj < 0.1 & !is.na(padj), 
                                   "< 0.1", 
                                   "> 0.1"), 
               Input=Input) 

    return(df)
}


# Initialize lists of result tables with (resList) or without (shr_resList) shrinkage
resList <- ddsList 

shr_resList <- ddsList  

for (x in TvC) {


    # Extract results
    resList[[x]] <- as.data.frame(results(ddsList[[x]], 
                                          contrast=Contrast, 
                                          alpha=alpha))

    shr_resList[[x]] <- as.data.frame(shr_ddsList[[x]])

    # clean the data frame
    resList[[x]] <- Sig_fn(resList[[x]], x)
    shr_resList[[x]] <- Sig_fn(shr_resList[[x]], x)
    
}


```


## Exploratory data analysis of the extracted log2FoldChange tables



```{r exploring_foldtable}

# No shrinkage summary
summary(resList)
head(resList[['TPM']])
head(resList[['Counts']])

# Shrinkage summary
summary(shr_resList)
head(shr_resList[['TPM']])
head(shr_resList[['Counts']])

```


## Exploring mean-difference relationship with MA plots

- Red dashed lines: log2FoldChange = -1 and 1

```{r MAplot}

# Set ylim: has to adjusted by users depending on data 
yl <- c(-20, 20)

# Set min log2 fold change of interest 
mLog <- c(-1, 1)

# Define a function creating an MA plot
MA_fn <- function(List, Shr) {

    MAList <- ddsList 

    for (i in 1:2) {

        MAplot <- ggplot(List[[i]], 
                         aes(x=baseMean,
                             y=log2FoldChange,
                             color=FDR)) + geom_point() + scale_x_log10() + theme_bw() + scale_color_manual(values=c("blue", "grey")) + ggtitle(paste("MA plot:", names(List)[i], "Input with", Shr)) + ylim(yl[1], yl[2])+ geom_hline(yintercept=c(mLog[1], mLog[2]), linetype="dashed", color="red")

        MAList[[i]] <- MAplot

    }

    return(MAList)

}


# Create MA plots with or without shrinkage and store in a list
MA <- MA_fn(resList, "No Shrinkage")
shr_MA <- MA_fn(shr_resList, "Shrinkage")

```

## Visualizing MA plots

- **x-axis**: expression level 
- **y-axis**: fold change (log2FoldChange)
- **Upper**: TPM with (left) or without (right) shrinkage
- **Lower**: Counts with (left) or without (right) shrinkage


```{r presenting_MAplots}

# TPM with or without shrinkage
grid.arrange(MA[[1]], shr_MA[[1]], nrow=1)

# TPM with or without shrinkage
grid.arrange(MA[[2]], shr_MA[[2]], nrow=1)

```



## Exploring distribution of false discovery rate (FDR)

- Distribution of adjusted p-val was presented by input type
- Black dashed line: FDR = 0.1

```{r FDR_distribution}


# Combining total data table 
res <- rbind(resList[['TPM']], resList[['Counts']])
res$Input <- factor(res$Input, levels=TvC)  # TvC=c("TPM", "Counts")

# Create a plot presenting distribution of FDR
ggplot(res,
       aes(x=padj, color=Input)) + 
geom_density(size=1) + 
theme_bw() +
ggtitle("Distribution of False Discovery Rate (FDR)") + 
xlab("Adjusted P-Value") + 
ylab("Density") + 
geom_vline(xintercept=alpha, 
           color="black", 
           linetype="dashed",
           size=1) + 
scale_x_continuous(breaks=seq(0, 1, by=0.1)) 
```

## Volcano plots

- **x-axis**: expression level
- **y-axis**: log odds (larger log odds = more promising) 
- Red dashed lines: log2FoldChange = -1 and 1 

```{r volcano_plot}

# Set a basic volcano plot function 
Volcano_fn <- function(df, Label=NULL) {


ggplot(res, 
       aes(x=log2FoldChange,
           y= -log10(padj),
           color=FDR,
           label=Label)) + 
geom_point() +
facet_grid(Input ~.) + 
theme_bw() +
scale_color_manual(values=c("blue", "grey")) + 
ggtitle("Volcano Plot") + 
ylab("-log10(FDR)") + 
theme(strip.text.y=element_text(size=12)) + 
geom_vline(xintercept=c(mLog[1], mLog[2]), 
           color="red", 
           linetype="dashed", 
           size=1) 
}

# Display the volcano plots by input
Volcano_fn(res)


```


## Volcano plots with promising genes 

- **Log odds** threshold (y-axis): **> 20**

```{r high_logodds_genes}

# Assign log odds threshold 
LogOddsCut=20 


# Add a column indicating high log odds genes 
res <- res %>% 
    mutate(Label=ifelse(-log10(padj) > LogOddsCut, 
                                   Gene, 
                                   "")) 

# Display the genes with volcano plots
Volcano_fn(res, Label=res$Label) + geom_text_repel(color="black")

```



## Exploring distribution of log2FoldChange by input type

- Black dashed lines: log2FoldChange = -1 and 1 

```{r L2FC_distribution}



ggplot(res[res$FDR == "< 0.1", ],  # Subset rows with FDR < alpha 
       aes(x=log2FoldChange,
           color=Input)) + 
geom_density(size=1) +
theme_bw() + 
ylab("Density") + 
geom_vline(xintercept=c(mLog[1], mLog[2]), 
           color="black",
           linetype="dashed", size=1) +
ggtitle("Distribution of Log2 Folds by Input Type") 



```



## Exploring expression profiling 

- VST transformed count matrices are extracted from vsd objects and cleaned with thresholds set at FDR and log2FoldChange
- **lowfdrList**: a list of matrices filtered with **FDR < alpha**
- **highfoldList**: a list of matrices filtered with **FDR < alpha** AND **log2FoldChange > user's minimum threshold (minLog)**
- In this analysis, minLog was set to 1
- This transformed and filtered count matrices will be used to as inputs for creating profiling heatmaps


```{r transcription_profiling}



# Initialize a list 
lowfdrList <- ddsList   # A list for normalized counts matrix with FDR below alpha
highfoldList <- ddsList  # A list for normalized counts with log2foldchange over minLog



for (x in TvC) {

    # Create filtering vectors with alpha and log2foldchange
    BelowAlpha <- resList[[x]]$FDR == "< 0.1"
    overmLog <- resList[[x]]$log2FoldChange > mLog[2]  # mLog has been set to c(-1, 1) previously

    # Extract transformed counts from vsd objects (TPM[['vsd']] and Counts[['vsd']]) 
    if (x == "TPM") {

        vsdCounts <- assay(TPM[['vsd']])
    
    } else {

        vsdCounts <- assay(Counts[['vsd']])
    }

    
    # Update the normalized count matrix with FDR below alpha
    lowfdrList[[x]] <- vsdCounts[BelowAlpha, ]
    highfoldList[[x]] <- vsdCounts[BelowAlpha & overminLog, ]

    summary(lowfdrList[[x]])
    summary(highfoldList[[x]])
}

# Initialize map lists 
lowfdrMap <- ddsList
highfoldMap <- ddsList 

# Set a function creating a heatmap
ProfileHeatmap_fn <- function(inputmatrix, Title1, Title2, Title3=NULL) {
    
    as.ggplot(pheatmap(inputmatrix, 
             annotation=HeatmapAnno, 
             main=paste("Transcription Profiles with", 
                        Title1, 
                        "input and", 
                        Title2, 
                        alpha, 
                        Title3)))
}

# Create and save heatmaps
for (x in TvC) {

    lowfdrMap[[x]] <- ProfileHeatmap_fn(lowfdrList[[x]],
                                        Title1=x, 
                                        Title2="FDR <")

    highfoldMap[[x]] <- ProfileHeatmap_fn(highfoldList[[x]], 
                                          Title1=x, 
                                          Title2="FDR <",
                                          Title3=paste("Plus Minimum Log2 Fold Change >", mLog[2])) 
}
```



## Session Info 

```{r sessionInfo}
sessionInfo()
```
